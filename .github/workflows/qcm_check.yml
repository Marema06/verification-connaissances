name: CI Backend + Frontend with Ollama

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  build-and-test:
    runs-on: ubuntu-22.04
    timeout-minutes: 30  # Important pour le tÃ©lÃ©chargement du modÃ¨le

    steps:
      - name: ğŸ“¥ Checkout repository
        uses: actions/checkout@v4

      # ===== INSTALL OLLAMA =====
      - name: ğŸ¦™ Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          echo "$HOME/.local/bin" >> $GITHUB_PATH
          ollama pull llama3
          ollama list

      # ===== BACKEND SETUP =====
      - name: ğŸ Setup Python 3.10
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          cache: 'pip'

      - name: ğŸš€ Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install gunicorn

      - name: ğŸ—ï¸ Start Services
        env:
          OLLAMA_URL: "http://localhost:11434"
          OLLAMA_MODEL: "llama3"
        run: |
          # DÃ©marrer Ollama
          nohup ollama serve > ollama.log 2>&1 &
          echo $! > ollama.pid

          # Attendre qu'Ollama soit prÃªt
          for i in {1..10}; do
            if curl -s $OLLAMA_URL/api/tags; then
              echo "âœ… Ollama ready"
              break
            fi
            sleep 3
            echo "â³ Waiting for Ollama ($i/10)..."
          done

          # DÃ©marrer Flask
          cd backend
          nohup gunicorn --bind 0.0.0.0:5000 --workers 1 --timeout 120 app:app > flask.log 2>&1 &
          echo $! > flask.pid

          # VÃ©rifier Flask
          for i in {1..10}; do
            if curl -s http://localhost:5000/healthz; then
              echo "âœ… Flask ready"
              break
            fi
            sleep 2
            echo "â³ Waiting for Flask ($i/10)..."
          done

      # ===== FRONTEND SETUP =====
      - name: ğŸ”§ Setup Node.js 18.x
        uses: actions/setup-node@v3
        with:
          node-version: '18.x'
          cache: 'npm'

      - name: ğŸ“¦ Install frontend deps
        working-directory: frontend
        run: npm ci

      - name: ğŸ—ï¸ Build Angular
        working-directory: frontend
        run: npm run build --if-present

      # ===== TESTS =====
      - name: ğŸ§ª Test LLM Integration
        run: |
          curl -X POST http://localhost:5000/generate_qcm \
            -H "Content-Type: application/json" \
            -d '{"code_block":"def factorial(n):\n    return 1 if n == 0 else n * factorial(n-1)","author":"github_action"}'

      # ===== CLEANUP =====
      - name: ğŸ§¹ Stop services
        if: always()
        run: |
          if [ -f backend/flask.pid ]; then
            kill -9 $(cat backend/flask.pid) || true
          fi
          if [ -f ollama.pid ]; then
            kill -9 $(cat ollama.pid) || true
          fi
