name: CI with Ollama (CPU Mode)

on:
  push:
    branches: [ main ]
  workflow_dispatch:

jobs:
  build-and-test:
    runs-on: ubuntu-22.04
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      # 1. Installer Ollama en mode CPU
      - name: Install Ollama (CPU Mode)
        run: |
          # Installation
          curl -fsSL https://ollama.com/install.sh | OLLAMA_NOBLAS=1 sh

          # Configuration pour mode CPU
          echo "OLLAMA_NOBLAS=1" >> $GITHUB_ENV
          echo "HOME=/github/home" >> $GITHUB_ENV

          # Démarrer le serveur en arrière-plan
          nohup ollama serve > ollama.log 2>&1 &
          echo $! > ollama.pid
          sleep 10

      # 2. Télécharger un modèle optimisé CPU
      - name: Download CPU-optimized model
        run: |
          ollama pull orca-mini
          ollama list

      # 3. Setup Python/Flask
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Python dependencies
        run: |
          pip install -r requirements.txt
          pip install gunicorn

      # 4. Démarrer Flask avec timeout étendu
      - name: Start Flask
        env:
          OLLAMA_URL: "http://localhost:11434"
          OLLAMA_MODEL: "orca-mini"
        run: |
          nohup gunicorn --timeout 300 --bind 0.0.0.0:5000 --workers 1 backend.app:app > flask.log 2>&1 &
          echo $! > flask.pid

          # Attendre que Flask soit prêt
          for i in {1..15}; do
            if curl -s http://localhost:5000/healthz; then
              echo "Flask ready"
              break
            else
              echo "Waiting for Flask ($i/15)..."
              sleep 5
            fi
          done

      # 5. Test d'intégration complet
      - name: Test QCM Generation
        run: |
          # Générer un QCM
          RESPONSE=$(curl -s -X POST http://localhost:5000/generate_qcm \
            -H "Content-Type: application/json" \
            -d '{"code":"def hello():\\n    print(\\"Hello World\\")"}')

          echo "Response: $RESPONSE"

          # Extraire l'ID du QCM
          QCM_ID=$(echo $RESPONSE | jq -r '.qcm_id')

          if [ -z "$QCM_ID" ]; then
            echo "Erreur: Aucun QCM_ID reçu"
            exit 1
          fi

          echo "QCM ID: $QCM_ID"

          # Récupérer le QCM
          curl -s http://localhost:5000/qcm/$QCM_ID | jq .

          # Vérifier que des questions existent
          QUESTION_COUNT=$(curl -s http://localhost:5000/qcm/$QCM_ID | jq '.questions | length')

          if [ "$QUESTION_COUNT" -lt 1 ]; then
            echo "Erreur: Aucune question générée"
            exit 1
          fi

          echo "Success: $QUESTION_COUNT questions générées"

      # 6. Cleanup
      - name: Stop services
        if: always()
        run: |
          echo "=== Nettoyage des processus ==="

          # Arrêt Flask
          if [ -f flask.pid ]; then
            echo "Arrêt de Flask..."
            kill -9 $(cat flask.pid) 2>/dev/null || true
            rm -f flask.pid
          fi

          # Arrêt Ollama
          if [ -f ollama.pid ]; then
            echo "Arrêt d'Ollama..."
            kill -9 $(cat ollama.pid) 2>/dev/null || true
            rm -f ollama.pid
          fi

          # Nettoyage supplémentaire
          pkill -f "ollama serve" 2>/dev/null || true
          pkill -f "gunicorn" 2>/dev/null || true

          echo "=== Logs Ollama ==="
          cat ollama.log || echo "Pas de log Ollama"

          echo "=== Logs Flask ==="
          cat flask.log || echo "Pas de log Flask"

      # 7. Upload logs for debugging - CORRECTION ICI
      - name: Upload logs
        if: always()
        uses: actions/upload-artifact@v4  # Changé de v3 à v4
        with:
          name: service-logs
          path: |
            ollama.log
            flask.log
